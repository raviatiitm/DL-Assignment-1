{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mK4lc5w462V",
        "outputId": "69a86864-67c9-48b4-bee8-8d3a10480a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=1cd6bf60803bb8b3a85eae8f2ede63df0d33a5dd9d3f0904844005760a9ddf63\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "9LwMF0uC499F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "VlOe2KCh4-i9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnqc8vnI5IEe",
        "outputId": "2796033d-e935-4484-e343-dfe9d15c28dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_network:\n",
        "    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=32,epochs=1,               activation_func=\"sigmoid\",learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"nesterov\",weight_init=\"random\",loss='cross_entropy'):\n",
        "\n",
        "        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\n",
        "\n",
        "        np.random.seed(10)\n",
        "        self.gradient={}\n",
        "        for i in range(hidden_layers+2):\n",
        "            self.gradient[\"W\"+str(i)]=i;\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.batch = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.activation_func = activation_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.optimizer = optimizer\n",
        "        for i in range(hidden_layers+2):\n",
        "            self.gradient[\"b\"+str(i)]=i;\n",
        "        self.weight_init = weight_init\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\n",
        "        layers = self.layers.copy()\n",
        "        self.activations = []\n",
        "        self.loss = loss\n",
        "        self.activation_gradients = []\n",
        "        self.optimizer_list={'gradient_descent':self.gradient_descent,'sgd':self.sgd,'nesterov':self.nesterov,'nadam':self.nadam,'adam':self.adam,'momentum':self.momentum,'rmsprop':self.rmsprop}\n",
        "        self.weights_gradients = []\n",
        "        self.biases_gradients = []\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        n=len(layers)\n",
        "        for i in range(n-1):\n",
        "            if self.weight_init == 'random':\n",
        "                a=np.random.normal(0,0.5,(layers[i],layers[i+1]))\n",
        "                self.weights.append(a)\n",
        "                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\n",
        "            else :\n",
        "                std = np.sqrt(2/(layers[i]*layers[i+1]))\n",
        "                a=np.random.normal(0,std,(layers[i],layers[i+1]))\n",
        "                self.weights.append(a)\n",
        "                self.biases.append(np.random.normal(0,std,(layers[i+1])))\n",
        "            v1=np.zeros(layers[i])\n",
        "            self.activations.append(v1)\n",
        "            v2=np.zeros(layers[i+1])\n",
        "            self.activation_gradients.append(v2)\n",
        "            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\n",
        "            self.biases_gradients.append(v2)\n",
        "        self.activations.append(np.zeros(layers[-1]))\n",
        "        self.optimizer_list[optimizer](self.x_train,self.y_train)\n",
        "            \n",
        "\n",
        "    def sigmoid(self,activations):\n",
        "        res = []\n",
        "        for z in activations:\n",
        "            if z<-40:\n",
        "                res.append(0.0)\n",
        "            elif z>40:\n",
        "                res.append(1.0)\n",
        "            else:\n",
        "                res.append(1/(1+np.exp(-z)))\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def tanh(self,activations):\n",
        "        res = []\n",
        "        for z in activations:\n",
        "            if z<-20:\n",
        "                res.append(-1.0)\n",
        "            elif z>20:\n",
        "                res.append(1.0)\n",
        "            else:\n",
        "                temp=(np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "                res.append(temp)\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def relu(self,activations):\n",
        "        res = []\n",
        "        for i in activations:\n",
        "            if i>0:\n",
        "                res.append(i)\n",
        "            else:\n",
        "                res.append(0)\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def softmax(self,activations):\n",
        "        tot = 0\n",
        "        res=[]\n",
        "        for z in activations:\n",
        "            tot += np.exp(z)\n",
        "        res=np.asarray([np.exp(z)/tot for z in activations])\n",
        "        return res\n",
        "    \n",
        "\n",
        "    def forward_propagation(self,x,y,weights,biases):\n",
        "        n = len(self.layers)\n",
        "        pre_activation=[]\n",
        "        for i in range(n-2):\n",
        "            pre_activation.append(i)\n",
        "        self.activations[0] = x\n",
        "        for i in range(n-2):\n",
        "            if self.activation_func == \"sigmoid\":\n",
        "                s=self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] =s\n",
        "            elif self.activation_func == \"tanh\":\n",
        "                t=self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] =t\n",
        "            elif self.activation_func == \"relu\":\n",
        "                r=self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] = r\n",
        "        temp=self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])\n",
        "        self.activations[n-1] = temp      \n",
        "        if self.loss == \"cross_entropy\":      \n",
        "          return -(np.log2(self.activations[-1][y])) \n",
        "        elif self.loss == \"mse\":\n",
        "          y_onehot = np.zeros(self.output_dim)\n",
        "          y_onehot[y] = 1\n",
        "          return np.sum(np.square(self.activations[-1] - y_onehot))\n",
        "\n",
        "\n",
        "    def grad_w(self,i):\n",
        "        gw=np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\n",
        "        return gw\n",
        "\n",
        "\n",
        "    def grad_b(self,i):\n",
        "        gb=self.activation_gradients[i]\n",
        "        return gb\n",
        "\n",
        "\n",
        "    def backward_propagation(self,x,y,weights,biases):\n",
        "        y_onehot = np.zeros(self.output_dim)\n",
        "        y_onehot[y] = 1\n",
        "        if self.loss == \"cross_entropy\": \n",
        "          self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\n",
        "        elif self.loss == \"mse\":\n",
        "          temp_vec = 2 * (self.activations[-1] - y_onehot) * (self.activations[-1])\n",
        "          for i in range(len(self.activations[-1])):\n",
        "            i_onehot=np.zeros(self.output_dim)\n",
        "            i_onehot[i]=1\n",
        "            self.activation_gradients[-1][i] = np.dot(temp_vec,(i_onehot - np.asarray([self.activations[-1][i]]*self.output_dim)))\n",
        "        n = len(self.layers)\n",
        "        for i in range(n-2,-1,-1):\n",
        "            gw=self.grad_w(i)\n",
        "            self.weights_gradients[i] += gw\n",
        "            gb= self.grad_b(i)\n",
        "            self.biases_gradients[i] +=gb\n",
        "            if i!=0:\n",
        "                val1=self.activation_gradients[i]\n",
        "                value = np.matmul(weights[i],val1)\n",
        "                if self.activation_func == \"sigmoid\":\n",
        "                    val= value * self.activations[i] * (1-self.activations[i])\n",
        "                    self.activation_gradients[i-1] = val\n",
        "                elif self.activation_func == \"tanh\":\n",
        "                    val=value * (1-np.square(self.activations[i]))\n",
        "                    self.activation_gradients[i-1] = val\n",
        "                elif self.activation_func == \"relu\":\n",
        "                    res = []\n",
        "                    for k in self.activations[i]:\n",
        "                        ans=1.0 if k>0 else 0.0\n",
        "                        res.append(ans)\n",
        "                    res = np.asarray(res)\n",
        "                    self.activation_gradients[i-1] = value * res\n",
        "                   \n",
        "\n",
        "    def gradient_descent(self,x_train,y_train):\n",
        "        grads=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            grads.append(i)\n",
        "        for i in range(self.epochs):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss = 0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "                wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients =bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if temp == 0 or index == x_train.shape[0]:\n",
        "                    n=len(self.weights)\n",
        "                    for j in range(n):\n",
        "                        w_g=self.learning_rate * self.weights_gradients[j]\n",
        "                        self.weights[j] -= w_g\n",
        "                        b_g=self.learning_rate * self.biases_gradients[j]\n",
        "                        self.biases[j] -= b_g\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients =bg\n",
        "                index += 1 \n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               temp=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=temp\n",
        "            temp1=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(temp1,3)\n",
        "            temp2=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(temp2,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "    def sgd(self,x_train,y_train):\n",
        "        grads=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            grads.append(i)\n",
        "        t=self.epochs\n",
        "        for i in range(t):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if  temp== 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        temp=self.learning_rate * self.weights_gradients[j]\n",
        "                        self.weights[j] -= temp\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients =bg\n",
        "                index +=1   \n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               temp=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=temp\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "            \n",
        "    def momentum(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "        n=self.epochs\n",
        "\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if  temp== 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        v1=self.learning_rate * self.weights_gradients[j]\n",
        "                        v_w =(self.decay_rate * prev_gradients_w[j] +v1)\n",
        "                        v2= self.learning_rate * self.biases_gradients[j]\n",
        "                        v_b = (self.decay_rate * prev_gradients_b[j] + v2)\n",
        "                        self.weights[j] -= v_w\n",
        "                        self.biases[j] -= v_b\n",
        "                        prev_gradients_w[j] = v_w\n",
        "                        prev_gradients_b[j] = v_b\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "\n",
        "    def nesterov(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            for j in range(len(self.weights)):\n",
        "              temp=self.weights[j] -  (self.decay_rate * prev_gradients_w[j])\n",
        "              self.weights[j]=temp\n",
        "              self.biases[j] =self.biases[j] -  self.decay_rate * prev_gradients_b[j]\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if temp == 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        temp1=self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j]\n",
        "                        prev_gradients_w[j] =temp1\n",
        "                        temp2= self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j]               \n",
        "                        prev_gradients_b[j] =  temp2\n",
        "                                        \n",
        "                        self.weights[j] -= prev_gradients_w[j]\n",
        "                        self.biases[j] -= prev_gradients_b[j]\n",
        "                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\n",
        "                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                       wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index += 1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "            \n",
        "    def rmsprop(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "        eps = 1e-2\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index%self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    for j in range(len(self.weights)):\n",
        "                        t1=(1-self.beta) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta * prev_gradients_w[j] +t1)\n",
        "                        t2=(1-self.beta) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta * prev_gradients_b[j] +t2)\n",
        "                        denom_w=(self.weights_gradients[j] /(np.sqrt(v_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * denom_w\n",
        "                        denom_b=(self.biases_gradients[j] /(np.sqrt(v_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * denom_b\n",
        "                        prev_gradients_w[j] = v_w\n",
        "                        prev_gradients_b[j] = v_b\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients=wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "\n",
        "    def adam(self,x_train,y_train):\n",
        "        m_prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        m_prev_gradients_w=temp1\n",
        "        m_prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        m_prev_gradients_b=temp2\n",
        "\n",
        "        v_prev_gradients_w=[]\n",
        "        temp3=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp3.append(0*i)\n",
        "        v_prev_gradients_w=temp3\n",
        "        v_prev_gradients_b=[]\n",
        "        temp4=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp4.append(0*i)\n",
        "        v_prev_gradients_b=temp4\n",
        "        iter = 1\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            eps = 1e-2\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss +=val \n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index%self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    s=len(self.weights)\n",
        "                    for j in range(s):\n",
        "                        p1=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j]) + p1\n",
        "                        p2=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j]) + p2\n",
        "                        p3=(1-self.beta2) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j]) + p3\n",
        "                        p4=(1-self.beta2) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j]) + p4\n",
        "                        denom1=(1-(self.beta1)**iter)\n",
        "                        m_hat_w = (m_w)/ denom1\n",
        "                        m_hat_b = (m_b)/denom1\n",
        "                        denom2=(1-(self.beta2)**iter)\n",
        "                        v_hat_w = (v_w)/ denom2\n",
        "                        v_hat_b = (v_b)/denom2\n",
        "                        t1=(m_hat_w/(np.sqrt(v_hat_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * t1\n",
        "                        t2=(m_hat_b/(np.sqrt(v_hat_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * t2\n",
        "                        v1=m_prev_gradients_w[j]\n",
        "                        m_prev_gradients_w[j] = m_w\n",
        "                        m_prev_gradients_b[j] = m_b\n",
        "                        v2=v_prev_gradients_w[j]\n",
        "                        v_prev_gradients_w[j] = v_w\n",
        "                        v_prev_gradients_b[j] = v_b\n",
        "                        wg=[]\n",
        "                        for i in (self.weights_gradients):\n",
        "                           wg.append(0*i)\n",
        "                        self.weights_gradients = wg\n",
        "                        bg=[]\n",
        "                        for i in (self.biases_gradients):\n",
        "                          bg.append(0*i)\n",
        "                        self.biases_gradients=bg\n",
        "                    iter += 1\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "        \n",
        "\n",
        "    def nadam(self,x_train,y_train):\n",
        "        m_prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        m_prev_gradients_w=temp1\n",
        "        m_prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        m_prev_gradients_b=temp2\n",
        "\n",
        "        v_prev_gradients_w=[]\n",
        "        temp3=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp3.append(0*i)\n",
        "        v_prev_gradients_w=temp3\n",
        "        v_prev_gradients_b=[]\n",
        "        temp4=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp4.append(0*i)\n",
        "        v_prev_gradients_b=temp4\n",
        "        iter = 1\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            eps = 1e-2\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index % self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    s=len(self.weights)\n",
        "                    for j in range(s):\n",
        "                        p1=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j]) + p1\n",
        "                        p2=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j]) + p2\n",
        "                        p3=(1-self.beta2) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j]) + p3\n",
        "                        p4=(1-self.beta2) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j]) + p4\n",
        "                        denom1=(1-(self.beta1)**iter)\n",
        "                        m_hat_w = (m_w)/ denom1\n",
        "                        m_hat_b = (m_b)/denom1\n",
        "                        denom2=(1-(self.beta2)**iter)\n",
        "                        v_hat_w = (v_w)/ denom2\n",
        "                        v_hat_b = (v_b)/denom2\n",
        "                        t3=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_dash_w = self.beta1 * m_hat_w + t3\n",
        "                        t4=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_dash_b = self.beta1 * m_hat_b + t4\n",
        "                        t1=(m_dash_w/(np.sqrt(v_hat_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * t1\n",
        "                        t2=(m_dash_b/(np.sqrt(v_hat_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * t2\n",
        "                        v1=m_prev_gradients_w[j]\n",
        "                        m_prev_gradients_w[j] = m_w\n",
        "                        v2=m_prev_gradients_b[j]\n",
        "                        m_prev_gradients_b[j] = m_b\n",
        "                        v_prev_gradients_w[j] = v_w\n",
        "                        v_prev_gradients_b[j] = v_b\n",
        "                        wg=[]\n",
        "                        for i in (self.weights_gradients):\n",
        "                           wg.append(0*i)\n",
        "                        self.weights_gradients = wg\n",
        "                        bg=[]\n",
        "                        for i in (self.biases_gradients):\n",
        "                          bg.append(0*i)\n",
        "                        self.biases_gradients=bg\n",
        "                    iter += 1\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "    \n",
        "    def calculate_accuracy(self,X,Y,flag=False):\n",
        "        count = 0\n",
        "        for i in range(len(X)):\n",
        "            if self.predict(X[i]) == Y[i]:\n",
        "                count+=1\n",
        "            if flag:\n",
        "              self.conf_matrix[self.predict(X[i])][Y[i]]+=1\n",
        "        if flag:\n",
        "          wandb.log({'Confusion matrix': wandb.plots.HeatMap(self.actual_labels, self.predicted_labels, self.conf_matrix, show_text=True)})\n",
        "        return count/len(X)\n",
        "\n",
        "    def predict(self,x):\n",
        "        n=len(self.layers)\n",
        "        x = x.ravel()\n",
        "        self.activations[0] = x\n",
        "        for i in range(n-2):\n",
        "            if self.activation_func == \"sigmoid\":\n",
        "                val=self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "            elif self.activation_func == \"tanh\":\n",
        "                val=self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "            elif self.activation_func == \"relu\":\n",
        "                val=self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "\n",
        "        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\n",
        "\n",
        "        return np.argmax(self.activations[-1])"
      ],
      "metadata": {
        "id": "TD4bqGXy5SNo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=3e-3,batch_size = 64,epochs=3,\n",
        "                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.9,loss=\"cross_entropy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x2VYVR04-gc",
        "outputId": "507688de-0e51-4d33-8349-c85b1bee94c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  0.8724798116893993   accuracy =  0.826    validation loss=  0.6899052297527021   validation accuaracy=  0.816\n",
            "Epoch--- 2   loss =  0.591963252037559   accuracy =  0.857    validation loss=  0.587711560147939   validation accuaracy=  0.847\n",
            "Epoch--- 3   loss =  0.5355915003189532   accuracy =  0.868    validation loss=  0.5468003807348609   validation accuaracy=  0.862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=3e-3,batch_size = 64,epochs=3,\n",
        "                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.9,loss=\"mse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMaR55LVyf1H",
        "outputId": "9f9ebded-1ba4-4ac2-8919-ebcb3fcab2ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  0.32646973018922176   accuracy =  0.839    validation loss=  0.24180666089717037   validation accuaracy=  0.829\n",
            "Epoch--- 2   loss =  0.21795579690597403   accuracy =  0.85    validation loss=  0.2258066981118539   validation accuaracy=  0.841\n",
            "Epoch--- 3   loss =  0.19817767856436438   accuracy =  0.839    validation loss=  0.24834383598390236   validation accuaracy=  0.825\n"
          ]
        }
      ]
    }
  ]
}