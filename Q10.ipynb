{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW-oH2JW7AQa",
        "outputId": "05955155-5d3c-494b-972b-46ce9b7f6957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=49e271b5757d9b568893ab805d80bb0e653057b502d697a97c2856a7e12923c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "f_PBpvU-7IBF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "xOLbj41s7Kft"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project='DL_Assignment1',entity='cs22m069')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "P0wnsdx58Qew",
        "outputId": "5974c1f1-1323-453d-b312-e5142d26b971"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230319_164129-whum3z8d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m069/DL_Assignment1/runs/whum3z8d' target=\"_blank\">sweet-dust-137</a></strong> to <a href='https://wandb.ai/cs22m069/DL_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m069/DL_Assignment1' target=\"_blank\">https://wandb.ai/cs22m069/DL_Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m069/DL_Assignment1/runs/whum3z8d' target=\"_blank\">https://wandb.ai/cs22m069/DL_Assignment1/runs/whum3z8d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs22m069/DL_Assignment1/runs/whum3z8d?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb384d3bdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.run.name = 'op_{}_act_{}_lr_{}_layer_{}_bth_{}'.format('nadam','tanh',2e-3,2,64)"
      ],
      "metadata": {
        "id": "69KyVNpQ8VX8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lifgmfcd7NnF",
        "outputId": "447512a4-9e01-4593-fdcb-d35d7f020935"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_network:\n",
        "    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=32,epochs=1,               activation_func=\"sigmoid\",learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"nesterov\",weight_init=\"random\",loss='cross_entropy'):\n",
        "\n",
        "        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\n",
        "\n",
        "        np.random.seed(10)\n",
        "        self.gradient={}\n",
        "        for i in range(hidden_layers+2):\n",
        "            self.gradient[\"W\"+str(i)]=i;\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.batch = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.activation_func = activation_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.optimizer = optimizer\n",
        "        for i in range(hidden_layers+2):\n",
        "            self.gradient[\"b\"+str(i)]=i;\n",
        "        self.weight_init = weight_init\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\n",
        "        layers = self.layers.copy()\n",
        "        self.activations = []\n",
        "        self.loss = loss\n",
        "        self.activation_gradients = []\n",
        "        self.optimizer_list={'gradient_descent':self.gradient_descent,'sgd':self.sgd,'nesterov':self.nesterov,'nadam':self.nadam,'adam':self.adam,'momentum':self.momentum,'rmsprop':self.rmsprop}\n",
        "        self.weights_gradients = []\n",
        "        self.biases_gradients = []\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        n=len(layers)\n",
        "        for i in range(n-1):\n",
        "            if self.weight_init == 'random':\n",
        "                a=np.random.normal(0,0.5,(layers[i],layers[i+1]))\n",
        "                self.weights.append(a)\n",
        "                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\n",
        "            else :\n",
        "                std = np.sqrt(2/(layers[i]*layers[i+1]))\n",
        "                a=np.random.normal(0,std,(layers[i],layers[i+1]))\n",
        "                self.weights.append(a)\n",
        "                self.biases.append(np.random.normal(0,std,(layers[i+1])))\n",
        "            v1=np.zeros(layers[i])\n",
        "            self.activations.append(v1)\n",
        "            v2=np.zeros(layers[i+1])\n",
        "            self.activation_gradients.append(v2)\n",
        "            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\n",
        "            self.biases_gradients.append(v2)\n",
        "        self.activations.append(np.zeros(layers[-1]))\n",
        "        self.optimizer_list[optimizer](self.x_train,self.y_train)\n",
        "            \n",
        "\n",
        "    def sigmoid(self,activations):\n",
        "        res = []\n",
        "        for z in activations:\n",
        "            if z<-40:\n",
        "                res.append(0.0)\n",
        "            elif z>40:\n",
        "                res.append(1.0)\n",
        "            else:\n",
        "                res.append(1/(1+np.exp(-z)))\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def tanh(self,activations):\n",
        "        res = []\n",
        "        for z in activations:\n",
        "            if z<-20:\n",
        "                res.append(-1.0)\n",
        "            elif z>20:\n",
        "                res.append(1.0)\n",
        "            else:\n",
        "                temp=(np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "                res.append(temp)\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def relu(self,activations):\n",
        "        res = []\n",
        "        for i in activations:\n",
        "            if i>0:\n",
        "                res.append(i)\n",
        "            else:\n",
        "                res.append(0)\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def softmax(self,activations):\n",
        "        tot = 0\n",
        "        res=[]\n",
        "        for z in activations:\n",
        "            tot += np.exp(z)\n",
        "        res=np.asarray([np.exp(z)/tot for z in activations])\n",
        "        return res\n",
        "    \n",
        "\n",
        "    def forward_propagation(self,x,y,weights,biases):\n",
        "        n = len(self.layers)\n",
        "        pre_activation=[]\n",
        "        for i in range(n-2):\n",
        "            pre_activation.append(i)\n",
        "        self.activations[0] = x\n",
        "        for i in range(n-2):\n",
        "            if self.activation_func == \"sigmoid\":\n",
        "                s=self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] =s\n",
        "            elif self.activation_func == \"tanh\":\n",
        "                t=self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] =t\n",
        "            elif self.activation_func == \"relu\":\n",
        "                r=self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] = r\n",
        "        temp=self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])\n",
        "        self.activations[n-1] = temp      \n",
        "        if self.loss == \"cross_entropy\":      \n",
        "          return -(np.log2(self.activations[-1][y])) \n",
        "        elif self.loss == \"mse\":\n",
        "          y_onehot = np.zeros(self.output_dim)\n",
        "          y_onehot[y] = 1\n",
        "          return np.sum(np.square(self.activations[-1] - y_onehot))\n",
        "\n",
        "\n",
        "    def grad_w(self,i):\n",
        "        gw=np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\n",
        "        return gw\n",
        "\n",
        "\n",
        "    def grad_b(self,i):\n",
        "        gb=self.activation_gradients[i]\n",
        "        return gb\n",
        "\n",
        "\n",
        "    def backward_propagation(self,x,y,weights,biases):\n",
        "        y_onehot = np.zeros(self.output_dim)\n",
        "        y_onehot[y] = 1\n",
        "        if self.loss == \"cross_entropy\": \n",
        "          self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\n",
        "        elif self.loss == \"mse\":\n",
        "          temp_vec = 2 * (self.activations[-1] - y_onehot) * (self.activations[-1])\n",
        "          for i in range(len(self.activations[-1])):\n",
        "            i_onehot=np.zeros(self.output_dim)\n",
        "            i_onehot[i]=1\n",
        "            self.activation_gradients[-1][i] = np.dot(temp_vec,(i_onehot - np.asarray([self.activations[-1][i]]*self.output_dim)))\n",
        "        n = len(self.layers)\n",
        "        for i in range(n-2,-1,-1):\n",
        "            gw=self.grad_w(i)\n",
        "            self.weights_gradients[i] += gw\n",
        "            gb= self.grad_b(i)\n",
        "            self.biases_gradients[i] +=gb\n",
        "            if i!=0:\n",
        "                val1=self.activation_gradients[i]\n",
        "                value = np.matmul(weights[i],val1)\n",
        "                if self.activation_func == \"sigmoid\":\n",
        "                    val= value * self.activations[i] * (1-self.activations[i])\n",
        "                    self.activation_gradients[i-1] = val\n",
        "                elif self.activation_func == \"tanh\":\n",
        "                    val=value * (1-np.square(self.activations[i]))\n",
        "                    self.activation_gradients[i-1] = val\n",
        "                elif self.activation_func == \"relu\":\n",
        "                    res = []\n",
        "                    for k in self.activations[i]:\n",
        "                        ans=1.0 if k>0 else 0.0\n",
        "                        res.append(ans)\n",
        "                    res = np.asarray(res)\n",
        "                    self.activation_gradients[i-1] = value * res\n",
        "                   \n",
        "\n",
        "    def gradient_descent(self,x_train,y_train):\n",
        "        grads=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            grads.append(i)\n",
        "        for i in range(self.epochs):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss = 0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "                wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients =bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if temp == 0 or index == x_train.shape[0]:\n",
        "                    n=len(self.weights)\n",
        "                    for j in range(n):\n",
        "                        w_g=self.learning_rate * self.weights_gradients[j]\n",
        "                        self.weights[j] -= w_g\n",
        "                        b_g=self.learning_rate * self.biases_gradients[j]\n",
        "                        self.biases[j] -= b_g\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients =bg\n",
        "                index += 1 \n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               temp=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=temp\n",
        "            temp1=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(temp1,3)\n",
        "            temp2=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(temp2,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "    def sgd(self,x_train,y_train):\n",
        "        grads=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            grads.append(i)\n",
        "        t=self.epochs\n",
        "        for i in range(t):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if  temp== 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        temp=self.learning_rate * self.weights_gradients[j]\n",
        "                        self.weights[j] -= temp\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients =bg\n",
        "                index +=1   \n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               temp=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=temp\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "            \n",
        "    def momentum(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "        n=self.epochs\n",
        "\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if  temp== 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        v1=self.learning_rate * self.weights_gradients[j]\n",
        "                        v_w =(self.decay_rate * prev_gradients_w[j] +v1)\n",
        "                        v2= self.learning_rate * self.biases_gradients[j]\n",
        "                        v_b = (self.decay_rate * prev_gradients_b[j] + v2)\n",
        "                        self.weights[j] -= v_w\n",
        "                        self.biases[j] -= v_b\n",
        "                        prev_gradients_w[j] = v_w\n",
        "                        prev_gradients_b[j] = v_b\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "\n",
        "    def nesterov(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            for j in range(len(self.weights)):\n",
        "              temp=self.weights[j] -  (self.decay_rate * prev_gradients_w[j])\n",
        "              self.weights[j]=temp\n",
        "              self.biases[j] =self.biases[j] -  self.decay_rate * prev_gradients_b[j]\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if temp == 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        temp1=self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j]\n",
        "                        prev_gradients_w[j] =temp1\n",
        "                        temp2= self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j]               \n",
        "                        prev_gradients_b[j] =  temp2\n",
        "                                        \n",
        "                        self.weights[j] -= prev_gradients_w[j]\n",
        "                        self.biases[j] -= prev_gradients_b[j]\n",
        "                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\n",
        "                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                       wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index += 1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "            \n",
        "    def rmsprop(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "        eps = 1e-2\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index%self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    for j in range(len(self.weights)):\n",
        "                        t1=(1-self.beta) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta * prev_gradients_w[j] +t1)\n",
        "                        t2=(1-self.beta) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta * prev_gradients_b[j] +t2)\n",
        "                        denom_w=(self.weights_gradients[j] /(np.sqrt(v_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * denom_w\n",
        "                        denom_b=(self.biases_gradients[j] /(np.sqrt(v_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * denom_b\n",
        "                        prev_gradients_w[j] = v_w\n",
        "                        prev_gradients_b[j] = v_b\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients=wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "\n",
        "    def adam(self,x_train,y_train):\n",
        "        m_prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        m_prev_gradients_w=temp1\n",
        "        m_prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        m_prev_gradients_b=temp2\n",
        "\n",
        "        v_prev_gradients_w=[]\n",
        "        temp3=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp3.append(0*i)\n",
        "        v_prev_gradients_w=temp3\n",
        "        v_prev_gradients_b=[]\n",
        "        temp4=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp4.append(0*i)\n",
        "        v_prev_gradients_b=temp4\n",
        "        iter = 1\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            eps = 1e-2\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss +=val \n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index%self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    s=len(self.weights)\n",
        "                    for j in range(s):\n",
        "                        p1=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j]) + p1\n",
        "                        p2=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j]) + p2\n",
        "                        p3=(1-self.beta2) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j]) + p3\n",
        "                        p4=(1-self.beta2) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j]) + p4\n",
        "                        denom1=(1-(self.beta1)**iter)\n",
        "                        m_hat_w = (m_w)/ denom1\n",
        "                        m_hat_b = (m_b)/denom1\n",
        "                        denom2=(1-(self.beta2)**iter)\n",
        "                        v_hat_w = (v_w)/ denom2\n",
        "                        v_hat_b = (v_b)/denom2\n",
        "                        t1=(m_hat_w/(np.sqrt(v_hat_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * t1\n",
        "                        t2=(m_hat_b/(np.sqrt(v_hat_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * t2\n",
        "                        v1=m_prev_gradients_w[j]\n",
        "                        m_prev_gradients_w[j] = m_w\n",
        "                        m_prev_gradients_b[j] = m_b\n",
        "                        v2=v_prev_gradients_w[j]\n",
        "                        v_prev_gradients_w[j] = v_w\n",
        "                        v_prev_gradients_b[j] = v_b\n",
        "                        wg=[]\n",
        "                        for i in (self.weights_gradients):\n",
        "                           wg.append(0*i)\n",
        "                        self.weights_gradients = wg\n",
        "                        bg=[]\n",
        "                        for i in (self.biases_gradients):\n",
        "                          bg.append(0*i)\n",
        "                        self.biases_gradients=bg\n",
        "                    iter += 1\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "        \n",
        "\n",
        "    def nadam(self,x_train,y_train):\n",
        "        m_prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        m_prev_gradients_w=temp1\n",
        "        m_prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        m_prev_gradients_b=temp2\n",
        "\n",
        "        v_prev_gradients_w=[]\n",
        "        temp3=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp3.append(0*i)\n",
        "        v_prev_gradients_w=temp3\n",
        "        v_prev_gradients_b=[]\n",
        "        temp4=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp4.append(0*i)\n",
        "        v_prev_gradients_b=temp4\n",
        "        iter = 1\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            eps = 1e-2\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index % self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    s=len(self.weights)\n",
        "                    for j in range(s):\n",
        "                        p1=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j]) + p1\n",
        "                        p2=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j]) + p2\n",
        "                        p3=(1-self.beta2) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j]) + p3\n",
        "                        p4=(1-self.beta2) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j]) + p4\n",
        "                        denom1=(1-(self.beta1)**iter)\n",
        "                        m_hat_w = (m_w)/ denom1\n",
        "                        m_hat_b = (m_b)/denom1\n",
        "                        denom2=(1-(self.beta2)**iter)\n",
        "                        v_hat_w = (v_w)/ denom2\n",
        "                        v_hat_b = (v_b)/denom2\n",
        "                        t3=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_dash_w = self.beta1 * m_hat_w + t3\n",
        "                        t4=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_dash_b = self.beta1 * m_hat_b + t4\n",
        "                        t1=(m_dash_w/(np.sqrt(v_hat_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * t1\n",
        "                        t2=(m_dash_b/(np.sqrt(v_hat_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * t2\n",
        "                        v1=m_prev_gradients_w[j]\n",
        "                        m_prev_gradients_w[j] = m_w\n",
        "                        v2=m_prev_gradients_b[j]\n",
        "                        m_prev_gradients_b[j] = m_b\n",
        "                        v_prev_gradients_w[j] = v_w\n",
        "                        v_prev_gradients_b[j] = v_b\n",
        "                        wg=[]\n",
        "                        for i in (self.weights_gradients):\n",
        "                           wg.append(0*i)\n",
        "                        self.weights_gradients = wg\n",
        "                        bg=[]\n",
        "                        for i in (self.biases_gradients):\n",
        "                          bg.append(0*i)\n",
        "                        self.biases_gradients=bg\n",
        "                    iter += 1\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuracy':val_acc})\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "    \n",
        "    def calculate_accuracy(self,X,Y,flag=False):\n",
        "        count = 0\n",
        "        for i in range(len(X)):\n",
        "            if self.predict(X[i]) == Y[i]:\n",
        "                count+=1\n",
        "            if flag:\n",
        "              self.conf_matrix[self.predict(X[i])][Y[i]]+=1\n",
        "        if flag:\n",
        "          wandb.log({'Confusion matrix': wandb.plots.HeatMap(self.actual_labels, self.predicted_labels, self.conf_matrix, show_text=True)})\n",
        "        return count/len(X)\n",
        "\n",
        "    def predict(self,x):\n",
        "        n=len(self.layers)\n",
        "        x = x.ravel()\n",
        "        self.activations[0] = x\n",
        "        for i in range(n-2):\n",
        "            if self.activation_func == \"sigmoid\":\n",
        "                val=self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "            elif self.activation_func == \"tanh\":\n",
        "                val=self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "            elif self.activation_func == \"relu\":\n",
        "                val=self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "\n",
        "        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\n",
        "\n",
        "        return np.argmax(self.activations[-1])"
      ],
      "metadata": {
        "id": "oR9SYPgk7YEe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=2e-3,batch_size= 64,epochs=4,\n",
        "                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY1dFB_i7Nll",
        "outputId": "2f6649a9-5cd6-4b3a-adee-31834a428969"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  0.4835084137160567   accuracy =  0.956    validation loss=  0.23237722583959722   validation accuaracy=  0.954\n",
            "Epoch--- 2   loss =  0.18539228740675118   accuracy =  0.972    validation loss=  0.1744333671987683   validation accuaracy=  0.964\n",
            "Epoch--- 3   loss =  0.13105919448574616   accuracy =  0.977    validation loss=  0.15716057462496122   validation accuaracy=  0.965\n",
            "Epoch--- 4   loss =  0.10077285078510066   accuracy =  0.981    validation loss=  0.15391748518090498   validation accuaracy=  0.969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The test accuracy is:\",nn.calculate_accuracy(x_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0EteoEk7Njm",
        "outputId": "cbf145df-d72e-44a9-fdef-c43d4af4279b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy is: 0.9698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=5e-3,batch_size= 32,epochs=7,\n",
        "                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUf9rV4m7Ng0",
        "outputId": "9633c343-2d07-4f39-b695-64acf1cce891"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  0.4234705637182152   accuracy =  0.963    validation loss=  0.21691041449613238   validation accuaracy=  0.958\n",
            "Epoch--- 2   loss =  0.1982155991277626   accuracy =  0.969    validation loss=  0.19622833178210858   validation accuaracy=  0.961\n",
            "Epoch--- 3   loss =  0.16354087164715445   accuracy =  0.972    validation loss=  0.21294414896804706   validation accuaracy=  0.962\n",
            "Epoch--- 4   loss =  0.14172612311617158   accuracy =  0.975    validation loss=  0.18689424417884634   validation accuaracy=  0.967\n",
            "Epoch--- 5   loss =  0.12834052997437118   accuracy =  0.975    validation loss=  0.22939953493994258   validation accuaracy=  0.965\n",
            "Epoch--- 6   loss =  0.11713228677135037   accuracy =  0.977    validation loss=  0.2670011008497196   validation accuaracy=  0.963\n",
            "Epoch--- 7   loss =  0.11115946428785059   accuracy =  0.975    validation loss=  0.28255951819559455   validation accuaracy=  0.961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=5e-3,batch_size= 32,epochs=7,\n",
        "                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuCDpMoI7Nec",
        "outputId": "423cfcea-4c6b-4dc8-d95b-541b127f4d93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  0.4234705637182152   accuracy =  0.963    validation loss=  0.21691041449613238   validation accuaracy=  0.958\n",
            "Epoch--- 2   loss =  0.1982155991277626   accuracy =  0.969    validation loss=  0.19622833178210858   validation accuaracy=  0.961\n",
            "Epoch--- 3   loss =  0.16354087164715445   accuracy =  0.972    validation loss=  0.21294414896804706   validation accuaracy=  0.962\n",
            "Epoch--- 4   loss =  0.14172612311617158   accuracy =  0.975    validation loss=  0.18689424417884634   validation accuaracy=  0.967\n",
            "Epoch--- 5   loss =  0.12834052997437118   accuracy =  0.975    validation loss=  0.22939953493994258   validation accuaracy=  0.965\n",
            "Epoch--- 6   loss =  0.11713228677135037   accuracy =  0.977    validation loss=  0.2670011008497196   validation accuaracy=  0.963\n",
            "Epoch--- 7   loss =  0.11115946428785059   accuracy =  0.975    validation loss=  0.28255951819559455   validation accuaracy=  0.961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,16,3,10,learning_rate=6e-3,batch_size= 32,epochs=5,\n",
        "                    activation_func=\"sigmoid\",optimizer=\"rmsprop\",weight_init=\"random\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v064lQME1itu",
        "outputId": "1e67a3d3-8aa8-4c8c-dded-2e7c4c1d8eea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  1.0700620715642712   accuracy =  0.903    validation loss=  0.5312810673880506   validation accuaracy=  0.897\n",
            "Epoch--- 2   loss =  0.4558863616636201   accuracy =  0.926    validation loss=  0.4167704747563025   validation accuaracy=  0.92\n",
            "Epoch--- 3   loss =  0.3741814532825798   accuracy =  0.932    validation loss=  0.39215008379322036   validation accuaracy=  0.923\n",
            "Epoch--- 4   loss =  0.3306393792559904   accuracy =  0.936    validation loss=  0.37679283068089015   validation accuaracy=  0.925\n",
            "Epoch--- 5   loss =  0.30162908091960283   accuracy =  0.938    validation loss=  0.377018339942539   validation accuaracy=  0.927\n"
          ]
        }
      ]
    }
  ]
}