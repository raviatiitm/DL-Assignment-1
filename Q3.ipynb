{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T3UvUxPjU25h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2P9w-u8VPHr",
        "outputId": "e115c1b0-eab9-42b8-af5b-a4056b74d180"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Neural_network:\n",
        "    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=32,epochs=1,activation_func=\"sigmoid\"\n",
        "           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"nesterov\",weight_init=\"random\"):\n",
        "\n",
        "        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\n",
        "\n",
        "        np.random.seed(10)\n",
        "        self.gradient={}\n",
        "        for i in range(hidden_layers+2):\n",
        "            self.gradient[\"W\"+str(i)]=i;\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.batch = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.activation_func = activation_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.optimizer = optimizer\n",
        "        for i in range(hidden_layers+2):\n",
        "            self.gradient[\"b\"+str(i)]=i;\n",
        "        self.weight_init = weight_init\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\n",
        "        layers = self.layers.copy()\n",
        "        self.activations = []\n",
        "        self.activation_gradients = []\n",
        "        self.optimizer_list={'gradient_descent':self.gradient_descent,'sgd':self.sgd,'nesterov':self.nesterov,'nadam':self.nadam,'adam':self.adam,'momentum':self.momentum,'rmsprop':self.rmsprop}\n",
        "        self.weights_gradients = []\n",
        "        self.biases_gradients = []\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        n=len(layers)\n",
        "        for i in range(n-1):\n",
        "            if self.weight_init == 'random':\n",
        "                a=np.random.normal(0,0.5,(layers[i],layers[i+1]))\n",
        "                self.weights.append(a)\n",
        "                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\n",
        "            else :\n",
        "                std = np.sqrt(2/(layers[i]*layers[i+1]))\n",
        "                a=np.random.normal(0,std,(layers[i],layers[i+1]))\n",
        "                self.weights.append(a)\n",
        "                self.biases.append(np.random.normal(0,std,(layers[i+1])))\n",
        "            v1=np.zeros(layers[i])\n",
        "            self.activations.append(v1)\n",
        "            v2=np.zeros(layers[i+1])\n",
        "            self.activation_gradients.append(v2)\n",
        "            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\n",
        "            self.biases_gradients.append(v2)\n",
        "        self.activations.append(np.zeros(layers[-1]))\n",
        "        self.optimizer_list[optimizer](self.x_train,self.y_train)\n",
        "            \n",
        "\n",
        "    def sigmoid(self,activations):\n",
        "        res = []\n",
        "        for z in activations:\n",
        "            if z<-40:\n",
        "                res.append(0.0)\n",
        "            elif z>40:\n",
        "                res.append(1.0)\n",
        "            else:\n",
        "                res.append(1/(1+np.exp(-z)))\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def tanh(self,activations):\n",
        "        res = []\n",
        "        for z in activations:\n",
        "            if z<-20:\n",
        "                res.append(-1.0)\n",
        "            elif z>20:\n",
        "                res.append(1.0)\n",
        "            else:\n",
        "                temp=(np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "                res.append(temp)\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def relu(self,activations):\n",
        "        res = []\n",
        "        for i in activations:\n",
        "            if i>0:\n",
        "                res.append(i)\n",
        "            else:\n",
        "                res.append(0)\n",
        "        res=np.asarray(res)\n",
        "        return res\n",
        "\n",
        "    def softmax(self,activations):\n",
        "        tot = 0\n",
        "        res=[]\n",
        "        for z in activations:\n",
        "            tot += np.exp(z)\n",
        "        res=np.asarray([np.exp(z)/tot for z in activations])\n",
        "        return res\n",
        "\n",
        "    def forward_propagation(self,x,y,weights,biases):\n",
        "        n = len(self.layers)\n",
        "        pre_activation=[]\n",
        "        for i in range(n-2):\n",
        "            pre_activation.append(i)\n",
        "        self.activations[0] = x\n",
        "        for i in range(n-2):\n",
        "            if self.activation_func == \"sigmoid\":\n",
        "                s=self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] =s\n",
        "            elif self.activation_func == \"tanh\":\n",
        "                t=self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] =t\n",
        "            elif self.activation_func == \"relu\":\n",
        "                r=self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\n",
        "                self.activations[i+1] = r\n",
        "        temp=self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])\n",
        "        self.activations[n-1] = temp      \n",
        "        return -(np.log2(self.activations[-1][y]))\n",
        "\n",
        "\n",
        "    def grad_w(self,i):\n",
        "        gw=np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\n",
        "        return gw\n",
        "\n",
        "\n",
        "    def grad_b(self,i):\n",
        "        gb=self.activation_gradients[i]\n",
        "        return gb\n",
        "\n",
        "\n",
        "    def backward_propagation(self,x,y,weights,biases):\n",
        "        y_onehot = np.zeros(self.output_dim)\n",
        "        y_onehot[y] = 1\n",
        "        self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\n",
        "        n = len(self.layers)\n",
        "        for i in range(n-2,-1,-1):\n",
        "            gw=self.grad_w(i)\n",
        "            self.weights_gradients[i] += gw\n",
        "            gb= self.grad_b(i)\n",
        "            self.biases_gradients[i] +=gb\n",
        "            if i!=0:\n",
        "                val1=self.activation_gradients[i]\n",
        "                value = np.matmul(weights[i],val1)\n",
        "                if self.activation_func == \"sigmoid\":\n",
        "                    val= value * self.activations[i] * (1-self.activations[i])\n",
        "                    self.activation_gradients[i-1] = val\n",
        "                elif self.activation_func == \"tanh\":\n",
        "                    val=value * (1-np.square(self.activations[i]))\n",
        "                    self.activation_gradients[i-1] = val\n",
        "                elif self.activation_func == \"relu\":\n",
        "                    res = []\n",
        "                    for k in self.activations[i]:\n",
        "                        ans=1.0 if k>0 else 0.0\n",
        "                        res.append(ans)\n",
        "                    res = np.asarray(res)\n",
        "                    self.activation_gradients[i-1] = value * res\n",
        "\n",
        "    def gradient_descent(self,x_train,y_train):\n",
        "        grads=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            grads.append(i)\n",
        "        for i in range(self.epochs):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss = 0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "                wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients =bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if temp == 0 or index == x_train.shape[0]:\n",
        "                    n=len(self.weights)\n",
        "                    for j in range(n):\n",
        "                        w_g=self.learning_rate * self.weights_gradients[j]\n",
        "                        self.weights[j] -= w_g\n",
        "                        b_g=self.learning_rate * self.biases_gradients[j]\n",
        "                        self.biases[j] -= b_g\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients =bg\n",
        "                index += 1 \n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               temp=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=temp\n",
        "            temp1=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(temp1,3)\n",
        "            temp2=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(temp2,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "    def sgd(self,x_train,y_train):\n",
        "        grads=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            grads.append(i)\n",
        "        t=self.epochs\n",
        "        for i in range(t):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if  temp== 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        temp=self.learning_rate * self.weights_gradients[j]\n",
        "                        self.weights[j] -= temp\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients =bg\n",
        "                index +=1   \n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               temp=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=temp\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "            \n",
        "    def momentum(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "        n=self.epochs\n",
        "\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if  temp== 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        v1=self.learning_rate * self.weights_gradients[j]\n",
        "                        v_w =(self.decay_rate * prev_gradients_w[j] +v1)\n",
        "                        v2= self.learning_rate * self.biases_gradients[j]\n",
        "                        v_b = (self.decay_rate * prev_gradients_b[j] + v2)\n",
        "                        self.weights[j] -= v_w\n",
        "                        self.biases[j] -= v_b\n",
        "                        prev_gradients_w[j] = v_w\n",
        "                        prev_gradients_b[j] = v_b\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "\n",
        "    def nesterov(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            for j in range(len(self.weights)):\n",
        "              temp=self.weights[j] -  (self.decay_rate * prev_gradients_w[j])\n",
        "              self.weights[j]=temp\n",
        "              self.biases[j] =self.biases[j] -  self.decay_rate * prev_gradients_b[j]\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg\n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                temp=index % self.batch\n",
        "                if temp == 0 or index == x_train.shape[0]:\n",
        "                    lst=[0*i for i in (self.weights_gradients)]\n",
        "                    for j in range(len(self.weights)):\n",
        "                        temp1=self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j]\n",
        "                        prev_gradients_w[j] =temp1\n",
        "                        temp2= self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j]               \n",
        "                        prev_gradients_b[j] =  temp2\n",
        "                                        \n",
        "                        self.weights[j] -= prev_gradients_w[j]\n",
        "                        self.biases[j] -= prev_gradients_b[j]\n",
        "                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\n",
        "                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                       wg.append(0*i)\n",
        "                    self.weights_gradients = wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index += 1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "            \n",
        "    def rmsprop(self,x_train,y_train):\n",
        "        prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        prev_gradients_w=temp1\n",
        "        prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        prev_gradients_b=temp2\n",
        "        eps = 1e-2\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index%self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    for j in range(len(self.weights)):\n",
        "                        t1=(1-self.beta) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta * prev_gradients_w[j] +t1)\n",
        "                        t2=(1-self.beta) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta * prev_gradients_b[j] +t2)\n",
        "                        denom_w=(self.weights_gradients[j] /(np.sqrt(v_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * denom_w\n",
        "                        denom_b=(self.biases_gradients[j] /(np.sqrt(v_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * denom_b\n",
        "                        prev_gradients_w[j] = v_w\n",
        "                        prev_gradients_b[j] = v_b\n",
        "                    wg=[]\n",
        "                    for i in (self.weights_gradients):\n",
        "                      wg.append(0*i)\n",
        "                    self.weights_gradients=wg\n",
        "                    bg=[]\n",
        "                    for i in (self.biases_gradients):\n",
        "                      bg.append(0*i)\n",
        "                    self.biases_gradients=bg\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "\n",
        "\n",
        "    def adam(self,x_train,y_train):\n",
        "        m_prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        m_prev_gradients_w=temp1\n",
        "        m_prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        m_prev_gradients_b=temp2\n",
        "\n",
        "        v_prev_gradients_w=[]\n",
        "        temp3=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp3.append(0*i)\n",
        "        v_prev_gradients_w=temp3\n",
        "        v_prev_gradients_b=[]\n",
        "        temp4=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp4.append(0*i)\n",
        "        v_prev_gradients_b=temp4\n",
        "        iter = 1\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            eps = 1e-2\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss +=val \n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index%self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    s=len(self.weights)\n",
        "                    for j in range(s):\n",
        "                        p1=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j]) + p1\n",
        "                        p2=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j]) + p2\n",
        "                        p3=(1-self.beta2) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j]) + p3\n",
        "                        p4=(1-self.beta2) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j]) + p4\n",
        "                        denom1=(1-(self.beta1)**iter)\n",
        "                        m_hat_w = (m_w)/ denom1\n",
        "                        m_hat_b = (m_b)/denom1\n",
        "                        denom2=(1-(self.beta2)**iter)\n",
        "                        v_hat_w = (v_w)/ denom2\n",
        "                        v_hat_b = (v_b)/denom2\n",
        "                        t1=(m_hat_w/(np.sqrt(v_hat_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * t1\n",
        "                        t2=(m_hat_b/(np.sqrt(v_hat_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * t2\n",
        "                        v1=m_prev_gradients_w[j]\n",
        "                        m_prev_gradients_w[j] = m_w\n",
        "                        m_prev_gradients_b[j] = m_b\n",
        "                        v2=v_prev_gradients_w[j]\n",
        "                        v_prev_gradients_w[j] = v_w\n",
        "                        v_prev_gradients_b[j] = v_b\n",
        "                        wg=[]\n",
        "                        for i in (self.weights_gradients):\n",
        "                           wg.append(0*i)\n",
        "                        self.weights_gradients = wg\n",
        "                        bg=[]\n",
        "                        for i in (self.biases_gradients):\n",
        "                          bg.append(0*i)\n",
        "                        self.biases_gradients=bg\n",
        "                    iter += 1\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "        \n",
        "\n",
        "    def nadam(self,x_train,y_train):\n",
        "        m_prev_gradients_w=[]\n",
        "        temp1=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp1.append(0*i)\n",
        "        m_prev_gradients_w=temp1\n",
        "        m_prev_gradients_b=[]\n",
        "        temp2=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp2.append(0*i)\n",
        "        m_prev_gradients_b=temp2\n",
        "\n",
        "        v_prev_gradients_w=[]\n",
        "        temp3=[]\n",
        "        for i in (self.weights_gradients):\n",
        "            temp3.append(0*i)\n",
        "        v_prev_gradients_w=temp3\n",
        "        v_prev_gradients_b=[]\n",
        "        temp4=[]\n",
        "        for i in (self.biases_gradients):\n",
        "            temp4.append(0*i)\n",
        "        v_prev_gradients_b=temp4\n",
        "        iter = 1\n",
        "        n=self.epochs\n",
        "        for i in range(n):\n",
        "            print('Epoch---',i+1,end=\" \")\n",
        "            loss = 0\n",
        "            val_loss=0\n",
        "            eps = 1e-2\n",
        "            wg=[]\n",
        "            for i in (self.weights_gradients):\n",
        "              wg.append(0*i)\n",
        "            self.weights_gradients = wg\n",
        "            bg=[]\n",
        "            for i in (self.biases_gradients):\n",
        "              bg.append(0*i)\n",
        "            self.biases_gradients=bg \n",
        "            index = 1\n",
        "            for x,y in zip(x_train,y_train):\n",
        "                x = x.ravel()\n",
        "                val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "                loss += val\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\n",
        "                condt=index % self.batch\n",
        "                if condt == 0 or index == x_train.shape[0]:\n",
        "                    s=len(self.weights)\n",
        "                    for j in range(s):\n",
        "                        p1=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j]) + p1\n",
        "                        p2=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j]) + p2\n",
        "                        p3=(1-self.beta2) * np.square(self.weights_gradients[j])\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j]) + p3\n",
        "                        p4=(1-self.beta2) * np.square(self.biases_gradients[j])\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j]) + p4\n",
        "                        denom1=(1-(self.beta1)**iter)\n",
        "                        m_hat_w = (m_w)/ denom1\n",
        "                        m_hat_b = (m_b)/denom1\n",
        "                        denom2=(1-(self.beta2)**iter)\n",
        "                        v_hat_w = (v_w)/ denom2\n",
        "                        v_hat_b = (v_b)/denom2\n",
        "                        t3=(1-self.beta1) * self.weights_gradients[j]\n",
        "                        m_dash_w = self.beta1 * m_hat_w + t3\n",
        "                        t4=(1-self.beta1) * self.biases_gradients[j]\n",
        "                        m_dash_b = self.beta1 * m_hat_b + t4\n",
        "                        t1=(m_dash_w/(np.sqrt(v_hat_w + eps)))\n",
        "                        self.weights[j] -= self.learning_rate * t1\n",
        "                        t2=(m_dash_b/(np.sqrt(v_hat_b + eps)))\n",
        "                        self.biases[j] -= self.learning_rate * t2\n",
        "                        v1=m_prev_gradients_w[j]\n",
        "                        m_prev_gradients_w[j] = m_w\n",
        "                        v2=m_prev_gradients_b[j]\n",
        "                        m_prev_gradients_b[j] = m_b\n",
        "                        v_prev_gradients_w[j] = v_w\n",
        "                        v_prev_gradients_b[j] = v_b\n",
        "                        wg=[]\n",
        "                        for i in (self.weights_gradients):\n",
        "                           wg.append(0*i)\n",
        "                        self.weights_gradients = wg\n",
        "                        bg=[]\n",
        "                        for i in (self.biases_gradients):\n",
        "                          bg.append(0*i)\n",
        "                        self.biases_gradients=bg\n",
        "                    iter += 1\n",
        "                index +=1\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\n",
        "               x=x.ravel()\n",
        "               val=self.forward_propagation(x,y,self.weights,self.biases)\n",
        "               val_loss+=val\n",
        "            cal_acc=self.calculate_accuracy(x_train,y_train)\n",
        "            acc=round(cal_acc,3)\n",
        "            cal_acc_cv=self.calculate_accuracy(self.x_cv,self.y_cv)\n",
        "            val_acc=round(cal_acc_cv,3)\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= '\n",
        "                  ,val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\n",
        "    \n",
        "    def calculate_accuracy(self,X,Y):\n",
        "        count = 0\n",
        "        n=len(X)\n",
        "        for i in range(n):\n",
        "            if self.predict(X[i]) == Y[i]:\n",
        "                count+=1\n",
        "        res=count/n\n",
        "        return res\n",
        "\n",
        "    def predict(self,x):\n",
        "        n=len(self.layers)\n",
        "        x = x.ravel()\n",
        "        self.activations[0] = x\n",
        "        for i in range(n-2):\n",
        "            if self.activation_func == \"sigmoid\":\n",
        "                val=self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "            elif self.activation_func == \"tanh\":\n",
        "                val=self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "            elif self.activation_func == \"relu\":\n",
        "                val=self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\n",
        "                self.activations[i+1] = val\n",
        "\n",
        "        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\n",
        "\n",
        "        return np.argmax(self.activations[-1])"
      ],
      "metadata": {
        "id": "skBKaoroV7Au"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=3e-3,batch_size = 64,epochs=2,\n",
        "                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZrT5Pt9V67G",
        "outputId": "52d2e7a4-5e0e-4451-f335-117e54baed2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  2.1835001220296606   accuracy =  0.701    validation loss=  1.6030870793283265   validation accuaracy=  0.698\n",
            "Epoch--- 2   loss =  1.3973373475006892   accuracy =  0.747    validation loss=  1.2725465361712314   validation accuaracy=  0.744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=2e-3,batch_size= 64,epochs=6,\n",
        "                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5h53U7aV64o",
        "outputId": "7b4f7849-2777-40f8-fa7b-d339481cc213"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch--- 1   loss =  2.9266520447609805   accuracy =  0.723    validation loss=  2.8829266189412888   validation accuaracy=  0.719\n",
            "Epoch--- 2   loss =  2.8697264424501805   accuracy =  0.742    validation loss=  2.863018858353165   validation accuaracy=  0.743\n",
            "Epoch--- 3   loss =  2.8556456835970243   accuracy =  0.753    validation loss=  2.852838424593447   validation accuaracy=  0.753\n",
            "Epoch--- 4   loss =  2.8472808443826882   accuracy =  0.76    validation loss=  2.8459623903687565   validation accuaracy=  0.76\n",
            "Epoch--- 5   loss =  2.841223712536368   accuracy =  0.765    validation loss=  2.8407056691004198   validation accuaracy=  0.765\n",
            "Epoch--- 6   loss =  2.836571673736401   accuracy =  0.769    validation loss=  2.836740850633813   validation accuaracy=  0.767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can run for other configurations same as above**\n"
      ],
      "metadata": {
        "id": "yfoQJJ8IbNWz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xWzIA7qVV62J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EV6mp4wQV6zh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}